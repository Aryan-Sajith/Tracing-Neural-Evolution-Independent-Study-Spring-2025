## Brief:

Here we explore a seminal advancement in neural networks designed to tackle sequential processing tasks like text parsing/generation. We use
Long Short‑Term Memory (LSTM), a landmark recurrent architecture that overcomes the vanishing‑gradient limitations of standard RNNs to learn extremely long‑range dependencies in sequential tasks such as text parsing and generation .


## Paper 1: [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

- **Date Published:** 1997
- **Authors** - Sepp Hochreiter, Jurgen Schmidhuber

