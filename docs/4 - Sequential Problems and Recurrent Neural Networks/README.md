## Brief:

Here we explore a seminal advancement in neural networks designed to tackle sequential processing tasks like text parsing/generation. We use
Long Short‑Term Memory (LSTM), a landmark recurrent architecture that overcomes the vanishing‑gradient limitations of standard RNNs to learn extremely long‑range dependencies in sequential tasks such as text parsing and generation .

